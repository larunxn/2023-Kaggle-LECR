{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<h3> Some materials needed for this step:</h3>\n    \n* Finetuned model from Step 1\n    \n* Top-K (I test on n_tops = 30) contents for each topic from Step 2\n    \n<h3> Note: The 0's fold is used for validation</h3>\n\n**Reference**: https://www.kaggle.com/code/ragnar123/lecr-xlm-roberta-base-baseline","metadata":{}},{"cell_type":"code","source":"# =========================================================================================\n# Libraries\n# =========================================================================================\nimport os\nimport gc\nimport time\nimport math\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.checkpoint import checkpoint\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\nfrom sklearn.model_selection import StratifiedGroupKFold\n%env TOKENIZERS_PARALLELISM=true\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# =========================================================================================\n# Configurations\n# =========================================================================================\nclass CFG:\n    print_freq = 500\n    num_workers = 4\n    model = '/kaggle/input/retrieve-model/paraphrase-multilingual-mpnet-base-v2-exp_fold0_epochs10'\n    tokenizer = AutoTokenizer.from_pretrained(model)\n    gradient_checkpointing = False\n    num_cycles = 0.5\n    warmup_ratio = 0.1\n    epochs = 3\n    encoder_lr = 1e-5\n    decoder_lr = 1e-4\n    eps = 1e-6\n    betas = (0.9, 0.999)\n    batch_size = 32\n    weight_decay = 0.01\n    max_grad_norm = 0.012\n    max_len = 512\n    n_folds = 5\n    seed = 42\n    \n# =========================================================================================\n# Seed everything for deterministic results\n# =========================================================================================\ndef seed_everything(cfg):\n    random.seed(cfg.seed)\n    os.environ['PYTHONHASHSEED'] = str(cfg.seed)\n    np.random.seed(cfg.seed)\n    torch.manual_seed(cfg.seed)\n    torch.cuda.manual_seed(cfg.seed)\n    torch.backends.cudnn.deterministic = True\n    \n# =========================================================================================\n# F2 score metric\n# =========================================================================================\ndef f2_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f2 = tp / (tp + 0.2 * fp + 0.8 * fn)\n    return round(f2.mean(), 4)\n\n# =========================================================================================\n# Data Loading\n# =========================================================================================\ndef read_data(cfg):\n    train = pd.read_csv('/kaggle/input/baseline-verify/train_top30_fold0_cv_with_groundtruth_final.csv')\n    train['title1'].fillna(\"Title does not exist\", inplace = True)\n    train['title2'].fillna(\"Title does not exist\", inplace = True)\n    correlations = pd.read_csv('/kaggle/input/learning-equality-curriculum-recommendations/correlations.csv')\n    \n    # Create feature column\n    train['text'] = train['title1'] + '[SEP]' + train['title2']\n    print(' ')\n    print('-' * 50)\n    print(f\"train.shape: {train.shape}\")\n    print(f\"correlations.shape: {correlations.shape}\")\n    return train, correlations\n\n\n# =========================================================================================\n# Get max length\n# =========================================================================================\ndef get_max_length(train, cfg):\n    lengths = []\n    for text in tqdm(train['text'].fillna(\"\").values, total = len(train)):\n        length = len(cfg.tokenizer(text, add_special_tokens = False)['input_ids'])\n        lengths.append(length)\n    cfg.max_len = max(lengths) + 2 # cls & sep\n    print(f\"max_len: {cfg.max_len}\")\n\n# =========================================================================================\n# Prepare input, tokenize\n# =========================================================================================\ndef prepare_input(text, cfg):\n    inputs = cfg.tokenizer.encode_plus(\n        text, \n        return_tensors = None, \n        add_special_tokens = True, \n        max_length = cfg.max_len,\n        pad_to_max_length = True,\n        truncation = True\n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype = torch.long)\n    return inputs\n\n# =========================================================================================\n# Custom dataset\n# =========================================================================================\nclass custom_dataset(Dataset):\n    def __init__(self, df, cfg):\n        self.cfg = cfg\n        self.texts = df['text'].values\n        self.labels = df['target'].values\n    def __len__(self):\n        return len(self.texts)\n    def __getitem__(self, item):\n        inputs = prepare_input(self.texts[item], self.cfg)\n        label = torch.tensor(self.labels[item], dtype = torch.float)\n        return inputs, label\n    \n# =========================================================================================\n# Collate function for training\n# =========================================================================================\ndef collate(inputs):\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    for k, v in inputs.items():\n        inputs[k] = inputs[k][:,:mask_len]\n    return inputs\n\n# =========================================================================================\n# Mean pooling class\n# =========================================================================================\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n    \n# =========================================================================================\n# Model\n# =========================================================================================\nclass custom_model(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states = True)\n        self.config.hidden_dropout = 0.0\n        self.config.hidden_dropout_prob = 0.0\n        self.config.attention_dropout = 0.0\n        self.config.attention_probs_dropout_prob = 0.0\n        self.model = AutoModel.from_pretrained(cfg.model, config = self.config)\n        if self.cfg.gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n        self.pool = MeanPooling()\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs.last_hidden_state\n        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n        return feature\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(feature)\n        return output\n    \n# =========================================================================================\n# Helper functions\n# =========================================================================================\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n# =========================================================================================\n# Train function loop\n# =========================================================================================\ndef train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg):\n    model.train()\n    scaler = torch.cuda.amp.GradScaler(enabled = True)\n    losses = AverageMeter()\n    start = end = time.time()\n    global_step = 0\n    for step, (inputs, target) in enumerate(train_loader):\n        inputs = collate(inputs)\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        target = target.to(device)\n        batch_size = target.size(0)\n        with torch.cuda.amp.autocast(enabled = True):\n            y_preds = model(inputs)\n            loss = criterion(y_preds.view(-1), target)\n        losses.update(loss.item(), batch_size)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n        global_step += 1\n        scheduler.step()\n        end = time.time()\n        if step % cfg.print_freq == 0 or step == (len(train_loader) - 1):\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Grad: {grad_norm:.4f}  '\n                  'LR: {lr:.8f}  '\n                  .format(epoch + 1, \n                          step, \n                          len(train_loader), \n                          remain = timeSince(start, float(step + 1) / len(train_loader)),\n                          loss = losses,\n                          grad_norm = grad_norm,\n                          lr = scheduler.get_lr()[0]))\n    return losses.avg\n\n# =========================================================================================\n# Valid function loop\n# =========================================================================================\ndef valid_fn(valid_loader, model, criterion, device, cfg):\n    losses = AverageMeter()\n    model.eval()\n    preds = []\n    start = end = time.time()\n    for step, (inputs, target) in enumerate(valid_loader):\n        inputs = collate(inputs)\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        target = target.to(device)\n        batch_size = target.size(0)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        loss = criterion(y_preds.view(-1), target)\n        losses.update(loss.item(), batch_size)\n        preds.append(y_preds.sigmoid().squeeze().to('cpu').numpy().reshape(-1))\n        end = time.time()\n        if step % cfg.print_freq == 0 or step == (len(valid_loader) - 1):\n            print('EVAL: [{0}/{1}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(step, \n                          len(valid_loader),\n                          loss = losses,\n                          remain = timeSince(start, float(step + 1) / len(valid_loader))))\n    predictions = np.concatenate(preds, axis = 0)\n    return losses.avg, predictions\n\n# =========================================================================================\n# Get best threshold\n# =========================================================================================\ndef get_best_threshold(x_val, val_predictions, correlations):\n    best_score = 0\n    best_threshold = None\n    for thres in np.arange(0.01, 1, 0.01):\n        x_val['predictions'] = np.where(val_predictions > thres, 1, 0)\n        x_val1 = x_val[x_val['predictions'] == 1]\n        x_val1 = x_val1.groupby(['topics_ids'])['content_ids'].unique().reset_index()\n        x_val1['content_ids'] = x_val1['content_ids'].apply(lambda x: ' '.join(x))\n        x_val1.columns = ['topic_id', 'predictions']\n        x_val0 = pd.Series(x_val['topics_ids'].unique())\n        x_val0 = x_val0[~x_val0.isin(x_val1['topic_id'])]\n        x_val0 = pd.DataFrame({'topic_id': x_val0.values, 'predictions': \"\"})\n        x_val_r = pd.concat([x_val1, x_val0], axis = 0, ignore_index = True)\n        x_val_r = x_val_r.merge(correlations, how = 'left', on = 'topic_id')\n        score = f2_score(x_val_r['content_ids'], x_val_r['predictions'])\n        if score > best_score:\n            best_score = score\n            best_threshold = thres\n    return best_score, best_threshold\n    \n# =========================================================================================\n# Train & Evaluate\n# =========================================================================================\ndef train_and_evaluate_one_fold(train, correlations, fold, cfg):\n    print(' ')\n    print(f\"========== fold: {fold} training ==========\")\n    # Split train & validation\n    x_train = train[train['fold'] != fold]\n    x_val = train[train['fold'] == fold]\n    valid_labels = x_val['target'].values\n    train_dataset = custom_dataset(x_train, cfg)\n    valid_dataset = custom_dataset(x_val, cfg)\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size = cfg.batch_size, \n        shuffle = True, \n        num_workers = cfg.num_workers, \n        pin_memory = True, \n        drop_last = True\n    )\n    valid_loader = DataLoader(\n        valid_dataset, \n        batch_size = cfg.batch_size, \n        shuffle = False, \n        num_workers = cfg.num_workers, \n        pin_memory = True, \n        drop_last = False\n    )\n    # Get model\n    model = custom_model(cfg)\n    model.to(device)\n    # Optimizer\n    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay = 0.0):\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_parameters = [\n            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n            'lr': encoder_lr, 'weight_decay': weight_decay},\n            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n            'lr': encoder_lr, 'weight_decay': 0.0},\n            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n            'lr': decoder_lr, 'weight_decay': 0.0}\n        ]\n        return optimizer_parameters\n    optimizer_parameters = get_optimizer_params(\n        model, \n        encoder_lr = cfg.encoder_lr, \n        decoder_lr = cfg.decoder_lr,\n        weight_decay = cfg.weight_decay\n    )\n    optimizer = AdamW(\n        optimizer_parameters, \n        lr = cfg.encoder_lr, \n        eps = cfg.eps, \n        betas = cfg.betas\n    )\n    num_train_steps = int(len(x_train) / cfg.batch_size * cfg.epochs)\n    num_warmup_steps = num_train_steps * cfg.warmup_ratio\n    # Scheduler\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps = num_warmup_steps, \n        num_training_steps = num_train_steps, \n        num_cycles = cfg.num_cycles\n        )\n    # Training & Validation loop\n    criterion = nn.BCEWithLogitsLoss(reduction = \"mean\")\n    best_score = 0\n    for epoch in range(cfg.epochs):\n        start_time = time.time()\n        # Train\n        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg)\n        # Validation\n        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device, cfg)\n        # Compute f2_score\n        score, threshold = get_best_threshold(x_val, predictions, correlations)\n        elapsed = time.time() - start_time\n        print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        print(f'Epoch {epoch+1} - Score: {score:.4f} - Threshold: {threshold:.5f}')\n        if score > best_score:\n            best_score = score\n            print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save(\n                {'model': model.state_dict(), 'predictions': predictions}, \n                f\"{cfg.model.replace('/', '-')}_fold{fold}_{cfg.seed}_ver5.pth\"\n                )\n            val_predictions = predictions\n    torch.cuda.empty_cache()\n    gc.collect()\n    # Get best threshold\n    best_score, best_threshold = get_best_threshold(x_val, val_predictions, correlations)\n    print(f'Our CV score is {best_score} using a threshold of {best_threshold}')\n    ","metadata":{"execution":{"iopub.status.busy":"2023-02-11T15:43:12.175260Z","iopub.execute_input":"2023-02-11T15:43:12.175931Z","iopub.status.idle":"2023-02-11T15:43:20.124394Z","shell.execute_reply.started":"2023-02-11T15:43:12.175760Z","shell.execute_reply":"2023-02-11T15:43:20.123420Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"env: TOKENIZERS_PARALLELISM=true\n","output_type":"stream"}]},{"cell_type":"code","source":"# Seed everything\nseed_everything(CFG)\n# Read data\ntrain, correlations = read_data(CFG)\n# Get max length\nget_max_length(train, CFG)\n# Train and evaluate one fold\ntrain_and_evaluate_one_fold(train, correlations, 0, CFG)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}