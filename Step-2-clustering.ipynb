{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<h3> Some materials needed for this step:</h3>\n    \n* Finetuned model from Step 1\n\n* Kfold dataset from Step 1\n    \n    \n<h3> Note:</h3>\n    \n* The 0's fold is used for validation\n\n* After generating Top-K candidates for training in the next step, we need to use the correlations file to add more label 1 in the training set, because although we get a very high max positive score at stage 1 for Top-K, some topics may have no label 1\n    \n**Reference**: https://www.kaggle.com/code/ragnar123/lecr-unsupervised-train-set-public ","metadata":{}},{"cell_type":"code","source":"# =========================================================================================\n# Libraries\n# =========================================================================================\n!pip -qqq install sentence-transformers\n\nimport os\nimport gc\nimport time\nimport math\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.checkpoint import checkpoint\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\nimport cupy as cp\nfrom cuml.metrics import pairwise_distances\nfrom cuml.neighbors import NearestNeighbors\n\nfrom sentence_transformers import SentenceTransformer\n\n%env TOKENIZERS_PARALLELISM=false\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# =========================================================================================\n# Configurations\n# =========================================================================================\nclass CFG:\n    num_workers = 4\n    model = '/kaggle/input/retrieve-model/paraphrase-multilingual-mpnet-base-v2-exp_fold0_epochs10'\n    tokenizer = AutoTokenizer.from_pretrained(model)\n    batch_size = 32\n    top_n = 30\n    seed = 42\n    \n# =========================================================================================\n# Data Loading\n# =========================================================================================\ndef read_data(cfg):\n    topics = pd.read_csv('/kaggle/input/learning-equality-curriculum-recommendations/topics.csv')\n    content = pd.read_csv('/kaggle/input/learning-equality-curriculum-recommendations/content.csv')\n    correlations = pd.read_csv('/kaggle/input/lcrs-kfolds/kfold_correlations.csv')\n    correlations = correlations[correlations.fold == 0]\n    # Fillna titles\n    topics['title'].fillna(\"\", inplace = True)\n    content['title'].fillna(\"\", inplace = True)\n    # Fillna descriptions\n    topics['description'].fillna(\"\", inplace = True)\n    content['description'].fillna(\"\", inplace = True)\n    # Sort by title length to make inference faster\n    topics['length'] = topics['title'].apply(lambda x: len(x))\n    content['length'] = content['title'].apply(lambda x: len(x))\n    topics.sort_values('length', inplace = True)\n    content.sort_values('length', inplace = True)\n    # Drop cols\n    topics.drop(['description', 'channel', 'category', 'level', 'language', 'parent', 'has_content', 'length'], axis = 1, inplace = True)\n    content.drop(['description', 'kind', 'language', 'text', 'copyright_holder', 'license', 'length'], axis = 1, inplace = True)\n    # Reset index\n    topics.reset_index(drop = True, inplace = True)\n    content.reset_index(drop = True, inplace = True)\n    print(' ')\n    print('-' * 50)\n    print(f\"topics.shape: {topics.shape}\")\n    print(f\"content.shape: {content.shape}\")\n    print(f\"correlations.shape: {correlations.shape}\")\n    return topics, content, correlations\n\n# =========================================================================================\n# Prepare input, tokenize\n# =========================================================================================\ndef prepare_input(text, cfg):\n    inputs = cfg.tokenizer.encode_plus(\n        text, \n        max_length = 64,\n        truncation=True,\n        return_tensors = None, \n        add_special_tokens = True, \n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype = torch.long)\n    return inputs\n\n# =========================================================================================\n# Unsupervised dataset\n# =========================================================================================\nclass uns_dataset(Dataset):\n    def __init__(self, df, cfg):\n        self.cfg = cfg\n        self.texts = df['title'].values\n    def __len__(self):\n        return len(self.texts)\n    def __getitem__(self, item):\n        inputs = prepare_input(self.texts[item], self.cfg)\n        return inputs\n    \n# =========================================================================================\n# Mean pooling class\n# =========================================================================================\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n# =========================================================================================\n# Unsupervised model\n# =========================================================================================\nclass uns_model(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.config = AutoConfig.from_pretrained(cfg.model)\n        self.model = AutoModel.from_pretrained(cfg.model, config = self.config)\n        self.pool = MeanPooling()\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_state = outputs.last_hidden_state\n        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n        return feature\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        return feature\n    \n# =========================================================================================\n# Get embeddings\n# =========================================================================================\ndef get_embeddings(loader, model, device):\n    model.eval()\n    preds = []\n    for step, inputs in enumerate(tqdm(loader)):\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    preds = np.concatenate(preds)\n    return preds\n\n# =========================================================================================\n# Get the amount of positive classes based on the total\n# =========================================================================================\ndef get_pos_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    int_true = np.array([len(x[0] & x[1]) / len(x[0]) for x in zip(y_true, y_pred)])\n    return round(np.mean(int_true), 5)\n\n# =========================================================================================\n# F2 Score \ndef f2_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f2 = tp / (tp + 0.2 * fp + 0.8 * fn)\n    return round(f2.mean(), 4)\n# ===========================================================================================\n\n\n# =========================================================================================\n# Build our training set\n# =========================================================================================\ndef build_training_set(topics, content, cfg):\n    # Create lists for training\n    topics_ids = []\n    content_ids = []\n    title1 = []\n    title2 = []\n    targets = []\n    folds = []\n    # Iterate over each topic\n    for k in tqdm(range(len(topics))):\n        row = topics.iloc[k]\n        topics_id = row['id']\n        topics_title = row['title']\n        predictions = row['predictions'].split(' ')\n        ground_truth = row['content_ids'].split(' ')\n        fold = row['fold']\n        for pred in predictions:\n            content_title = content.loc[pred, 'title']\n            topics_ids.append(topics_id)\n            content_ids.append(pred)\n            title1.append(topics_title)\n            title2.append(content_title)\n            folds.append(fold)\n            # If pred is in ground truth, 1 else 0\n            if pred in ground_truth:\n                targets.append(1)\n            else:\n                targets.append(0)\n    # Build training dataset\n    train = pd.DataFrame(\n        {'topics_ids': topics_ids, \n         'content_ids': content_ids, \n         'title1': title1, \n         'title2': title2, \n         'target': targets,\n         'fold' : folds}\n    )\n    # Release memory\n    del topics_ids, content_ids, title1, title2, targets\n    gc.collect()\n    return train\n    \n# =========================================================================================\n# Get neighbors\n# =========================================================================================\ndef get_neighbors(topics, content, cfg):\n    # Create topics dataset\n    topics_dataset = uns_dataset(topics, cfg)\n    # Create content dataset\n    content_dataset = uns_dataset(content, cfg)\n    # Create topics and content dataloaders\n    topics_loader = DataLoader(\n        topics_dataset, \n        batch_size = cfg.batch_size, \n        shuffle = False, \n        collate_fn = DataCollatorWithPadding(tokenizer = cfg.tokenizer, padding = 'longest'),\n        num_workers = cfg.num_workers, \n        pin_memory = True, \n        drop_last = False\n    )\n    content_loader = DataLoader(\n        content_dataset, \n        batch_size = cfg.batch_size, \n        shuffle = False, \n        collate_fn = DataCollatorWithPadding(tokenizer = cfg.tokenizer, padding = 'longest'),\n        num_workers = cfg.num_workers, \n        pin_memory = True, \n        drop_last = False\n        )\n    # Create unsupervised model to extract embeddings\n    model = uns_model(cfg)\n    model.to(device)\n    # Predict topics\n    topics_preds = get_embeddings(topics_loader, model, device)\n    content_preds = get_embeddings(content_loader, model, device)\n    # Transfer predictions to gpu\n    topics_preds_gpu = cp.array(topics_preds)\n    content_preds_gpu = cp.array(content_preds)\n    # Release memory\n    torch.cuda.empty_cache()\n    del topics_dataset, content_dataset, topics_loader, content_loader, topics_preds, content_preds\n    gc.collect()\n    # KNN model\n    print(' ')\n    print('Training KNN model...')\n    neighbors_model = NearestNeighbors(n_neighbors = cfg.top_n, metric = 'cosine')\n    neighbors_model.fit(content_preds_gpu)\n    indices = neighbors_model.kneighbors(topics_preds_gpu, return_distance = False)\n    predictions = []\n    for k in tqdm(range(len(indices))):\n        pred = indices[k]\n        p = ' '.join([content.loc[ind, 'id'] for ind in pred.get()])\n        predictions.append(p)\n    topics['predictions'] = predictions\n    # Release memory\n    del topics_preds_gpu, content_preds_gpu, neighbors_model, predictions, indices, model\n    gc.collect()\n    return topics, content\n     \n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-11T16:00:11.502037Z","iopub.execute_input":"2023-02-11T16:00:11.502867Z","iopub.status.idle":"2023-02-11T16:00:46.233238Z","shell.execute_reply.started":"2023-02-11T16:00:11.502744Z","shell.execute_reply":"2023-02-11T16:00:46.231925Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"env: TOKENIZERS_PARALLELISM=false\n","output_type":"stream"}]},{"cell_type":"code","source":"# Read data\ntopics, content, correlations = read_data(CFG)\n# Run nearest neighbors\ntopics, content = get_neighbors(topics, content, CFG)\n# Merge with target and comput max positive score\ntopics_test = topics.merge(correlations, how = 'inner', left_on = ['id'], right_on = ['topic_id'])\npos_score = get_pos_score(topics_test['content_ids'], topics_test['predictions'])\nprint(f'Our max positive score is {pos_score}')\n\nf_score = f2_score(topics_test['content_ids'], topics_test['predictions'])\nprint(f'Our f2_score is {f_score}')\n# We can delete correlations\ndel correlations\ngc.collect()\n# Set id as index for content\ncontent.set_index('id', inplace = True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-11T16:00:46.239886Z","iopub.execute_input":"2023-02-11T16:00:46.241450Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":" \n--------------------------------------------------\ntopics.shape: (76972, 2)\ncontent.shape: (154047, 2)\ncorrelations.shape: (12304, 3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2406 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fca516b98c042928a6ad7388a5c8c97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4814 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1fd2ced50264624b4b6cc7185d83e0c"}},"metadata":{}},{"name":"stdout","text":" \nTraining KNN model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/76972 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41c1d91a911d41cda7f18e3b8997003d"}},"metadata":{}}]},{"cell_type":"code","source":"# Build training set\nfull_correlations = pd.read_csv('/kaggle/input/lcrs-kfolds/kfold_correlations.csv')\ntopics_full = topics.merge(full_correlations, how = 'inner', left_on = ['id'], right_on = ['topic_id'])\ntopics_full['predictions'] = topics_full.apply(lambda x: ' '.join(list(set(x.predictions.split(' ') + x.content_ids.split(' ')))) \\\n                                               if x.fold != 0 else x.predictions, axis = 1)\ntrain = build_training_set(topics_full, content, CFG)\nprint(f'Our training set has {len(train)} rows')\n# Save train set to disk to train on another notebook\ntrain.to_csv('train_top30_fold0_cv_with_groundtruth_final.csv', index = False)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}